// index.ts
import { ChatOpenAI } from "@langchain/openai";
import { RecursiveCharacterTextSplitter } from "@langchain/textsplitters";
import { PyPDFLoader } from "@langchain/community/document_loaders/fs/pdf";
import { USearch } from "@langchain/community/vectorstores/usearch";
import { OllamaEmbeddings } from "@langchain/community/embeddings/ollama";
import { RetrievalQAChain, loadQAStuffChain } from "@langchain/chains";
import * as dotenv from "dotenv";

dotenv.config();

async function main() {
  console.log("ğŸš€ å¼€å§‹è¿è¡Œ Assessment æŠ¥å‘Š RAG POC...");

  // Step 1: åˆ›å»º Ollama LLM å®¢æˆ·ç«¯ï¼ˆå…¼å®¹ OpenAI æ¥å£ï¼‰
  const llm = new ChatOpenAI({
    configuration: {
      baseURL: process.env.OPENAI_API_BASE_URL, // http://localhost:11434/v1
      apiKey: process.env.OPENAI_API_KEY,       // ä»»æ„å€¼
    },
    model: process.env.MODEL_NAME || "llama3:8b",
    temperature: 0.7,
  });

  // Step 2: åŠ è½½ PDF æ–‡æ¡£
  console.log("ğŸ“„ æ­£åœ¨åŠ è½½ PDF æ–‡ä»¶...");
  const loader = new PyPDFLoader("documents/sample.pdf");
  const docs = await loader.load();
  console.log(`âœ… åŠ è½½å®Œæˆï¼Œå…± ${docs.length} é¡µ`);

  // Step 3: åˆ†å—
  const textSplitter = new RecursiveCharacterTextSplitter({
    chunkSize: 500,
    chunkOverlap: 50,
  });
  const splitDocs = await textSplitter.splitDocuments(docs);

  // Step 4: ä½¿ç”¨ Ollama Embeddings ç”Ÿæˆå‘é‡ï¼ˆéœ€å•ç‹¬å¯åŠ¨ embedding æ¨¡å‹ï¼‰
  // æ³¨æ„ï¼šOllama é»˜è®¤ embedding æ¨¡å‹æ˜¯ `nomic-embed-text`
  console.log("ğŸ§  æ­£åœ¨ç”Ÿæˆå‘é‡ embeddings...");
  const embeddings = new OllamaEmbeddings({
    model: "nomic-embed-text", // æ¨èç”¨äº embedding çš„è½»é‡æ¨¡å‹
  });

  // Step 5: åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆå†…å­˜ä¸­ï¼‰
  const vectorStore = await USearch.fromDocuments(splitDocs, embeddings);
  const retriever = vectorStore.asRetriever();

  // Step 6: åˆ›å»º RAG é—®ç­”é“¾
  const qaChain = new RetrievalQAChain({
    combineDocumentsChain: loadQAStuffChain(llm),
    retriever,
  });

  // Step 7: æµ‹è¯•é—®ç­”
  const question = "è¿™ä»½æŠ¥å‘Šçš„ä¸»è¦ç»“è®ºæ˜¯ä»€ä¹ˆï¼Ÿ";
  console.log(`\nâ“ é—®é¢˜: ${question}`);
  const answer = await qaChain.invoke({ query: question });
  console.log(`âœ… å›ç­”: ${answer.output}`);

  // Step 8: æµ‹è¯•æ€»ç»“åŠŸèƒ½
  console.log("\nğŸ“ æ­£åœ¨å°è¯•æ€»ç»“æŠ¥å‘Š...");
  const fullText = splitDocs.slice(0, 3).map((d) => d.pageContent).join("\n");
  const summaryPrompt = `
è¯·ç”¨ä¸­æ–‡æ€»ç»“ä»¥ä¸‹è¯„ä¼°æŠ¥å‘Šçš„æ ¸å¿ƒå†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
- è¯„ä¼°å¯¹è±¡
- ä¸»è¦å‘ç°
- é£é™©æˆ–é—®é¢˜
- å»ºè®®æˆ–ç»“è®º

å†…å®¹ï¼š
${fullText}
  `;
  const summary = await llm.invoke(summaryPrompt);
  console.log("ğŸ“‹ æ€»ç»“:\n", summary);
}

main().catch(console.error);